{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.10.7-py3-none-any.whl (10 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Using cached huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from transformers) (23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from transformers) (1.24.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yhunkim\\anaconda3\\envs\\chatgpt\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, pyyaml, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.10.7 huggingface-hub-0.13.3 pyyaml-6.0 tokenizers-0.13.2 transformers-4.27.4\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yhunkim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yhunkim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "import os\n",
    "# 불필요한 에러를 보여주지 않게 하는 os 설정\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, Conv1D, GlobalMaxPooling1D,Input, Reshape, GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate, LayerNormalization, MultiHeadAttention, TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "import sys\n",
    "from transformers import BertTokenizerFast, TFBertModel, TFBertForSequenceClassification, BertConfig\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification at 0x174cbf09b80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert)\n",
    "model = TFBertForSequenceClassification.from_pretrained(bert, num_labels=8)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열 전체 표시\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "len(model_w2v.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2094470795125646661\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4169138176\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3012637775583558840\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n",
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "True\n",
      "True\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# print(sys.version)\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# GPU 사용 가능한지 확인\n",
    "print(\"Num GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "# Cuda 및 GPU 동작 확인\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_built_with_gpu_support())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 소문자로 변환\n",
    "    text = text.lower()\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "    # 불용어 제거\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # 어간(Stemming), 표제어(Lemmatization) 추출\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # 텍스트로 다시 변환\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# model_w2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=500000)\n",
    "\n",
    "with zipfile.ZipFile(\"open.zip\", \"r\") as zip_f:\n",
    "    \n",
    "    print(zip_f.namelist())\n",
    "    file_list = zip_f.namelist()\n",
    "    df_list = []\n",
    "    for f in file_list:\n",
    "        \n",
    "        \n",
    "        with zip_f.open(f) as file:\n",
    "            df = pd.read_csv(file)\n",
    "            df_list.append(df)\n",
    "\n",
    "    submission_df = df_list[0]\n",
    "    train_df = df_list[2]\n",
    "    test_df = df_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47399 entries, 0 to 47398\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      47399 non-null  object\n",
      " 1   text    47399 non-null  object\n",
      " 2   label   47399 non-null  int32 \n",
      "dtypes: int32(1), object(2)\n",
      "memory usage: 925.9+ KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['label'])\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "train_df['label'] = np.asarray(label_encoder.transform(train_df['label']), dtype=np.int32)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>israel parliament start winter session jerusal...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>two third business owner say prepared outbreak...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>story highlightsred bull team principal christ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>final respect paid arafat palestinian pay last...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>steelers winning old fashioned way pound pitts...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>TRAIN_19995</td>\n",
       "      <td>toronto ottawa canadian technology firm shopif...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>TRAIN_19996</td>\n",
       "      <td>eu court annuls eu mln fine german bank update...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>TRAIN_19997</td>\n",
       "      <td>safran sa plan hire people world wide year fra...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>TRAIN_19998</td>\n",
       "      <td>central maine healthcare lewiston maine pandem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>TRAIN_19999</td>\n",
       "      <td>investor soon learn covid pandemic affected e ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                               text  label\n",
       "0      TRAIN_00000  israel parliament start winter session jerusal...      3\n",
       "1      TRAIN_00001  two third business owner say prepared outbreak...      2\n",
       "2      TRAIN_00002  story highlightsred bull team principal christ...      1\n",
       "3      TRAIN_00003  final respect paid arafat palestinian pay last...      3\n",
       "4      TRAIN_00004  steelers winning old fashioned way pound pitts...      1\n",
       "...            ...                                                ...    ...\n",
       "19995  TRAIN_19995  toronto ottawa canadian technology firm shopif...      2\n",
       "19996  TRAIN_19996  eu court annuls eu mln fine german bank update...      3\n",
       "19997  TRAIN_19997  safran sa plan hire people world wide year fra...      2\n",
       "19998  TRAIN_19998  central maine healthcare lewiston maine pandem...      0\n",
       "19999  TRAIN_19999  investor soon learn covid pandemic affected e ...      2\n",
       "\n",
       "[20000 rows x 3 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['text'][:20000].to_list()\n",
    "train_labels = train_df['label'][:20000].to_list()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing\n",
    "train_encodings = tokenizer.batch_encode_plus(train_texts, truncation=True, padding=True, max_length=64)\n",
    "val_encodings = tokenizer.batch_encode_plus(val_texts, truncation=True, padding=True, max_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainset-set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# validation-set\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 분류를 위한 추가 레이어 생성\n",
    "# dropout = tf.keras.layers.Dropout(0.3)\n",
    "# output = tf.keras.layers.Dense(8, activation='softmax')\n",
    "\n",
    "# # 입력층 생성\n",
    "# input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "# input_attention_masks = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name='attention_masks')\n",
    "\n",
    "# # BERT 입력\n",
    "# bert_output = bert({'input_ids': input_ids, 'attention_mask': input_attention_masks})[1]\n",
    "\n",
    "# # 추가 레이어 적용\n",
    "# dropout_output = dropout(bert_output)\n",
    "# outputs = output(dropout_output)\n",
    "\n",
    "# # 전체 모델 생성\n",
    "# model = tf.keras.Model(inputs=[input_ids, input_attention_masks], outputs=outputs)\n",
    "\n",
    "# 모델 컴파일\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "with tf.device('/GPU:0'):\n",
    "    # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    callback_earlystop = EarlyStopping(\n",
    "        monitor=\"val_accuracy\", \n",
    "        min_delta=0.001, # the threshold that triggers the termination (acc should at least improve 0.001)\n",
    "        patience=2)\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset.batch(64), epochs=10, batch_size=64,\n",
    "        validation_data=val_dataset.batch(64),\n",
    "        callbacks = [callback_earlystop]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sen = [sentence.split() for sentence in train_df['text']]\n",
    "test_sen = [sentence.split() for sentence in test_df['text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector_size = 200\n",
    "model_w2v = Word2Vec(train_sen, vector_size=Vector_size, window=5, min_count=5, workers=8, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "\n",
    "X = np.array([np.mean([model_w2v.wv[word] for word in sentence if word in model_w2v.wv.key_to_index.keys()], axis=0) for sentence in train_sen])\n",
    "test = np.array([np.mean([model_w2v.wv[word] for word in sentence if word in model_w2v.wv.key_to_index.keys()], axis=0) for sentence in test_sen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['label'].values\n",
    "X_train_emb, X_val_emb, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM 데이터셋 생성\n",
    "train_data = lgb.Dataset(X_train_emb, label=y_train)\n",
    "val_data = lgb.Dataset(X_val_emb, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n"
     ]
    }
   ],
   "source": [
    "# LGBMClassifier 모델 정의\n",
    "lgbm = LGBMClassifier(random_state=42, device='gpu')\n",
    "\n",
    "# 그리드 서치에서 탐색할 하이퍼파라미터 후보군\n",
    "param_grid = {\n",
    "    'num_leaves': [15, 31, 63],\n",
    "    'max_depth': [3, 4, 5, 6, -1],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'min_child_samples': [10, 20, 30],\n",
    "    'bagging_fraction': [0.5, 0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# 그리드 서치 수행\n",
    "grid_search = GridSearchCV(lgbm, param_grid=param_grid, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "# LightGBM 모델 학습\n",
    "    # lgb_model = lgb.train(params, train_data, valid_sets=[train_data, val_data], num_boost_round=1000, early_stopping_rounds=50, verbose_eval=50)\n",
    "    grid_search.fit(X_train_emb, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'bagging_fraction': 0.5, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_samples': 20, 'num_leaves': 63}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int64)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적 하이퍼파라미터 출력\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "\n",
    "# 최적 모델 저장\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(test)\n",
    "np.unique(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_df = pd.read_csv('submission.csv')\n",
    "submission_df['label'] = y_pred\n",
    "submission_df.to_csv('wor2vec_lightgbm_CV.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47399, 200)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(train_df['label']))\n",
    "y = to_categorical(train_df['label'], num_classes)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max([len(sentence) for sentence in X])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to have equal length for LSTM \n",
    "### word2vec 초기 설정시 vector_size \n",
    "# max_len = max([len(sentence) for sentence in X])\n",
    "# test_len = max([len(sentence) for sentence in test])\n",
    "X = pad_sequences(X, maxlen=Vector_size, padding='post')\n",
    "X_test = pad_sequences(test, maxlen=Vector_size, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 데이터셋 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_w2v.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yhunkim\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# LSTM 모델 정의\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(model_w2v.wv.key_to_index), output_dim=100, input_length=max_len))\n",
    "# model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(units=num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model = tf.keras.models.Sequential([\n",
    "#   tf.keras.layers.Embedding(input_dim=len(model_w2v.wv.key_to_index), output_dim=Vector_size, input_length=max_len),\n",
    "#   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "#   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "#   tf.keras.layers.Dense(64, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.5),\n",
    "#   tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "# 모델 하이퍼파라미터 설정\n",
    "\n",
    "\n",
    "DENSE_SIZE = 256\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# CNN 모델 구축\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(input_dim=len(model_w2v.wv.key_to_index), output_dim=Vector_size, input_length=max_len))\n",
    "# model.add(Conv1D(FILTERS, KERNEL_SIZE, activation='relu'))\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "# model.add(Dense(HIDDEN_SIZE, activation='relu'))\n",
    "# model.add(Dropout(DROPOUT_RATE))\n",
    "# model.add(Dense(num_classes, activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "MAX_SEQ_LEN = 200\n",
    "EMBEDDING_DIM = 300\n",
    "NUM_HEADS = 8\n",
    "DROPOUT_RATE = 0.1\n",
    "FFN_UNITS = 512\n",
    "\n",
    "# 입력 레이어\n",
    "inputs = Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32)\n",
    "\n",
    "# 임베딩 레이어\n",
    "embedding_layer = Embedding(input_dim=len(model_w2v.wv.key_to_index)+ 1, output_dim=EMBEDDING_DIM, input_length=MAX_SEQ_LEN)(inputs)\n",
    "dropout_layer = Dropout(DROPOUT_RATE)(embedding_layer)\n",
    "\n",
    "# 위치 임베딩\n",
    "position_embedding = tf.keras.layers.Embedding(input_dim=MAX_SEQ_LEN, output_dim=EMBEDDING_DIM)(tf.range(start=0, limit=MAX_SEQ_LEN, delta=1))\n",
    "position_embedding = dropout_layer + position_embedding\n",
    "\n",
    "# 멀티헤드 어텐션\n",
    "attention_layer = MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMBEDDING_DIM, dropout=DROPOUT_RATE)(position_embedding, position_embedding)\n",
    "\n",
    "# 어텐션과 임베딩을 연결\n",
    "addition_layer = tf.keras.layers.Add()([attention_layer, position_embedding])\n",
    "normalization_layer_1 = LayerNormalization(epsilon=1e-6)(addition_layer)\n",
    "\n",
    "# FeedForward 레이어\n",
    "feedforward_layer = TimeDistributed(Dense(units=FFN_UNITS, activation='relu'))(normalization_layer_1)\n",
    "feedforward_layer = Dropout(DROPOUT_RATE)(feedforward_layer)\n",
    "feedforward_layer = TimeDistributed(Dense(units=EMBEDDING_DIM))(feedforward_layer)\n",
    "\n",
    "# 어텐션과 FeedForward 레이어를 연결\n",
    "addition_layer = tf.keras.layers.Add()([feedforward_layer, normalization_layer_1])\n",
    "normalization_layer_2 = LayerNormalization(epsilon=1e-6)(addition_layer)\n",
    "\n",
    "# Global Max Pooling과 Global Average Pooling을 연결\n",
    "pooling_layer = Concatenate()([GlobalMaxPooling1D()(normalization_layer_2), GlobalAveragePooling1D()(normalization_layer_2)])\n",
    "dropout_layer = Dropout(DROPOUT_RATE)(pooling_layer)\n",
    "\n",
    "# 출력 레이어\n",
    "outputs = Dense(units=8, activation='softmax')(dropout_layer)\n",
    "\n",
    "# 모델 생성\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "Physical devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "# # GPU 사용 가능 여부 확인\n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#   try:\n",
    "#     # GPU 사용 설정\n",
    "#     for gpu in gpus:\n",
    "      \n",
    "#       print(gpu)\n",
    "#       tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "#   except RuntimeError as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True, save_weights_only=False, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(X_train, \n",
    "            y_train, \n",
    "            batch_size=64, \n",
    "            epochs=100, validation_data=(X_val, y_val),\n",
    "            use_multiprocessing=True, workers=8,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Word2vec\" + f\"_{100}\"\n",
    "model.save(f\"{model_name}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2127/2605 [=======================>......] - ETA: 13s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[138], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_test_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(test)\n\u001b[0;32m      2\u001b[0m y_test_pred\n",
      "File \u001b[1;32mc:\\Users\\yhunkim\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\yhunkim\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\keras\\engine\\training.py:2251\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2249\u001b[0m \u001b[39mfor\u001b[39;00m _, iterator \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39menumerate_epochs():  \u001b[39m# Single epoch.\u001b[39;00m\n\u001b[0;32m   2250\u001b[0m     \u001b[39mwith\u001b[39;00m data_handler\u001b[39m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m-> 2251\u001b[0m         \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m data_handler\u001b[39m.\u001b[39msteps():\n\u001b[0;32m   2252\u001b[0m             callbacks\u001b[39m.\u001b[39mon_predict_batch_begin(step)\n\u001b[0;32m   2253\u001b[0m             tmp_batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_function(iterator)\n",
      "File \u001b[1;32mc:\\Users\\yhunkim\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\keras\\engine\\data_adapter.py:1374\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_insufficient_data:  \u001b[39m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[0;32m   1373\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1374\u001b[0m original_spe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1375\u001b[0m can_run_full_execution \u001b[39m=\u001b[39m (\n\u001b[0;32m   1376\u001b[0m     original_spe \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1377\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_steps \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m original_spe\n\u001b[0;32m   1379\u001b[0m )\n\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[1;32mc:\\Users\\yhunkim\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnumpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    636\u001b[0m   \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 637\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_value()\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    638\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    639\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yhunkim\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\yhunkim\\Anaconda3\\envs\\chatgpt\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(test)\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
